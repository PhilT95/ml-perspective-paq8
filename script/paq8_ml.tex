\documentclass[11pt,usenames,dvipsnames]{beamer}
\usetheme{Madrid}
\usepackage{lmodern}
\usepackage{xcolor}



\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}



\setbeamercolor{block title alerted}{fg=Red, bg=Salmon!50!White}
\setbeamercolor{block body alerted}{bg=Salmon!20!White}


\setbeamercolor*{block title example}{fg=black, bg=Goldenrod}
\setbeamercolor{block body example}{bg=Goldenrod!30!White}

\setbeamercolor*{block title}{fg=White, bg=ForestGreen}
\setbeamercolor{block body}{bg=LimeGreen!30!White}

\setbeamertemplate{footline}{
\begin{flushright}
\Large\insertframenumber
\end{flushright}}
\setbeamertemplate{navigation symbols}{}
\author{Presentation by Phil Trommer}
\title{A Machine Learning Perspective on Predictive Coding with PAQ by Knoll \& de Freitas}
%\setbeamercovered{transparent} 
%\setbeamertemplate{navigation symbols}{} 
%\logo{} 
%\institute{} 
%\date{} 
%\subject{} 

% Inserts Section Introduction Slide
\AtBeginSection[] {
	\begin{frame}
		\frametitle{\insertsectionhead}
		\tableofcontents[currentsection,hideothersubsections]
	\end{frame}
}


\newcommand{\defText}[1]{\textcolor{Red}{#1}}





\colorlet{beamer@blendedblue}{ForestGreen}
\begin{document}



\begin{frame}
\titlepage
\end{frame}


\begin{frame}{Overview}
\tableofcontents
\end{frame}


\section{Introduction to PAQ}


\begin{frame}{Introduction to PAQ}
	\begin{block}{What is PAQ8}
			\begin{itemize}
				\item What is it?
				\item How does it work?
				\item What makes it so famous?
			\end{itemize}
	\end{block}
\end{frame}

\begin{frame}{Introduction to PAQ}

	
	\begin{alertblock}{What is PAQ?}<1->
		\begin{itemize}
			\item A lossless, open-source compression algorithm
			\item Brings high perfomance at the cost of increased memory usage and time consumption
			\item Related to PPM, is envisioned as PPMs improvement
		\end{itemize}
	\end{alertblock}
\end{frame}

	\begin{exampleblock}{Matt Mahoney}<2->
	\begin{minipage}[b]{0.70\linewidth}
		\begin{itemize}
			\item Born 1955
			\item Recieved Ph.D in computer science at Florida Tech in 2003
			\item Released PAQ1 on January 6, 2002
		\end{itemize}
	\hfill
	\end{minipage}
	\begin{minipage}[b]{0.28\linewidth}
		\includegraphics[scale=1.5]{files/matt.jpg}
	\end{minipage}
	\end{exampleblock}

\begin{frame}{Introduction to PAQ}
	\begin{exampleblock}{Principles of PAQ}
	
		\begin{itemize}
			\item Modeling combined with adaptive arithmetic encoding
			\item Open to additions and improvements
			\item Improves perfomance of PPM by including several predictors (i.e. models of data) 
			\item Combines the result of the predictors
		\end{itemize}
	\end{exampleblock}

\end{frame}

\begin{frame}{Introduction to PAQ}
	\begin{block}{Exemplary Predictors}
	\visible<1->{The order-$n$ context predictor
		\begin{itemize}
			\item Examines the last $n$ bits and counts the 1's and 0's
			\item Estimates probability whether next bit is 1 or 0 like PPM
		\end{itemize}	
		}		
	\visible<2->{
	The sparse context predictor
		\begin{itemize}
			\item 	Context consists of a specific amount of non-contiguous bytes before the current bit
			\item Useful for some binary files 
		\end{itemize}
		}
		
	\visible<3->{
	Whole word order-$n$ context
		\begin{itemize}
			\item Context is the latest $n$ whole words
			\item Non-alphabetical characters are ignored and upper- or lower case letters are viewed as the same
			\item Very useful for text files
		\end{itemize}
	}
	\end{block}
	

	
\end{frame}


\begin{frame}{Introduction to PAQ}
		\begin{exampleblock}{PAQ \& Predictors}<1->
		\begin{itemize}
			\item PAQ encoder looks at the beginning of input file for deciding which predictors are used
			\item Ways to combine predictions change through with the different versions
			\item Each predictor outputs a pair of bit counts $\left(n_0 , n_1\right)$ 
			\item Counts of each predictor are weighted with context length
		 	\item Those counts get summed up 
		\end{itemize}

	\end{exampleblock}
\end{frame}

\section{PAQ8L}
\begin{frame}{PAQ8L}
	\begin{exampleblock}{PAQ8 - What's new?}<1->
		\begin{itemize}
			\item Predictors don't produce a pair of bit counts anymore\\
			$\hookrightarrow$ those counts get weighted and normalized into the interval $[0,1]\subset\mathbb{R}$
			\item Instead  each predictor already outputs a probability
			\item \textit{paq8l} is a stable version of paq8, released by Matt Mahoney
		\end{itemize}
	\end{exampleblock}
	
		\begin{alertblock}{PAQ8L - Machine Learning Perspective}<2->
		\begin{itemize}
			\item paq8l is the version of PAQ used by \textit{Byron Knoll \& Nando de Freitas}
			\item They try to show the possibilities of PAQ beyond data compression
		\end{itemize}
	\end{alertblock}
\end{frame}

\subsection{Architecture}
\begin{frame}{Architecture}


	\begin{exampleblock}{Architecture of PAQ8}<1->
		\begin{itemize}
			\item Uses weighted combination of predictions from Large number of models
			\item Allows no-contiguous context matches
			\item paq8l uses \textbf{552} prediciton models
			\item Combines the output of them into a single one\\
			$\hookrightarrow$ Passes this through an \textit{adaptive probability map} (APM) before using the arithmetic coder
		\end{itemize}
	\end{exampleblock}
	\hfill
	
	\visible<2->{
	\begin{figure}[H]
		\centering
		\resizebox{0.95\textwidth}{!}{
			\input{files/paq8_architecture.pgf}
		}
		\caption{PAQ8 Architecture}
	\end{figure}
	}

	

\end{frame}

\subsection{Neural Network}
\begin{frame}{Neural network}
		
	\begin{alertblock}{Neurons of a neural network}<1->
	A \defText{neuron} takes one or more \defText{inputs} and gives an \defText{output}. \\
	Within the topic of machine learning, the neuron can be understood as a \defText{function}.
	\end{alertblock}

	\visible<2->{
		\begin{figure}[H]
			\centering
			\resizebox{0.275\textwidth}{!}{
				\input{files/neuron.pgf}
			}
			\caption{Neural network architecture}
		\end{figure}
	}
	

		
	
\end{frame}




\begin{frame}{Neural network}

	\begin{alertblock}{Layer in neural network}<1->
			A layer is a group of neurons.
	\end{alertblock}	
	
		
		
	\visible<2->{
	\begin{alertblock}{A neural network}
	Neural networks is defined by its layers:
	\begin{itemize}
		\item 1 input layer with $n$ inputs 
		\item 1 output layer with $k$ outputs
		\item $M$ layers between input and output layer (i.e. hidden layers)
		\item Layers can consist of different amounts of neurons
	\end{itemize}
	\end{alertblock}	
	
	}	
	
	\visible<3->{
	
	\begin{block}{General structure of neural network}
		Let it be an generic neural network with:	
		\begin{itemize}
			\item $x_1,...x_n$ inputs and $y_1,...,y_k$ outputs
			\item There are $M$ different layers between input and output
		\end{itemize}
	\end{block}
	}	

	
	


\end{frame}

\begin{frame}{Neural network}
		%Change picture to just a neuron
	\visible<1->{
	\begin{figure}[H]
		\centering
		\resizebox{!}{5cm}{
			\input{files/neural-network_architecture.pgf}
		}
		\caption{Neural network architecture}
	\end{figure}
	}
\end{frame}


\subsection{Model Mixer}
\begin{frame}{Model Mixer}
	\begin{exampleblock}{Model Mixer of paq8l}<1->
		\begin{itemize}
			\item Resembles a neural network with one hidden layer
			\item Hidden layer is between input and output layer\\
			$\hookrightarrow$ Artificial neurons take a set of weighted inputs\\
			Output is produced through activation function
		\end{itemize}
	\end{exampleblock}
	
	\begin{block}{Differences between paq8l and neural networks}<2->
		\begin{enumerate}
			\item Weights for first and second layers are learned online and independently for all nodes:
			\begin{itemize}
				\item Each node trained separately 
				\item reduces predictive cross-entropy error (unlike back propagation)
			\end{itemize}	
			\item Hidden nodes are partitioned into seven sets		 
		\end{enumerate}
	\end{block}
\end{frame}

\subsection{Mixture of Experts}
\subsection{Updating \& Filtering}
\section{Applications for PAQ8}
\section{References}





\end{document}